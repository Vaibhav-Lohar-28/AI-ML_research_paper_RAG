{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04. Agentic RAG Workflow (LangGraph)\n",
                "\n",
                "This is the professional, industry-grade Agentic RAG system.\n",
                "We use **LangGraph** to create a state machine that orchestrates the RAG process.\n",
                "\n",
                "**Agent Capabilities:**\n",
                "1.  **Retrieve**: Fetch documents.\n",
                "2.  **Grade**: Evaluate if retrieved documents are relevant to the question.\n",
                "3.  **Rewriter**: If documents are irrelevant, rewrite the query to be better.\n",
                "4.  **Generate**: Synthesize the answer.\n",
                "\n",
                "**Graph Flow:**\n",
                "`Start` -> `Retrieve` -> `Grade` -> `(Decide)` -> \n",
                "   - If Relevant -> `Generate` -> `End`\n",
                "   - If Irrelevant -> `Rewrite Query` -> `Retrieve` (Loop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List\n",
                "from typing_extensions import TypedDict\n",
                "\n",
                "from langchain_core.messages import BaseMessage, HumanMessage\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
                "from langchain_core.documents import Document\n",
                "\n",
                "from langchain_chroma import Chroma\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_ollama import ChatOllama\n",
                "\n",
                "from langgraph.graph import END, StateGraph\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "DB_DIR = \"data/chroma_db\"\n",
                "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
                "LLM_MODEL = \"llama3\"\n",
                "\n",
                "# Setup Components\n",
                "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={'device': 'cpu'})\n",
                "vector_store = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
                "retriever = vector_store.as_retriever()\n",
                "llm = ChatOllama(model=LLM_MODEL, temperature=0, format=\"json\") # JSON mode for grading\n",
                "llm_gen = ChatOllama(model=LLM_MODEL, temperature=0) # Normal mode for generation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define State"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GraphState(TypedDict):\n",
                "    \"\"\"\n",
                "    Represents the state of our graph.\n",
                "    \"\"\"\n",
                "    question: str\n",
                "    generation: str\n",
                "    documents: List[Document]\n",
                "    reformulated_count: int"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Nodes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def retrieve(state):\n",
                "    \"\"\"\n",
                "    Retrieve documents from vectorstore\n",
                "    \"\"\"\n",
                "    print(\"---RETRIEVE---\")\n",
                "    question = state[\"question\"]\n",
                "    documents = retriever.invoke(question)\n",
                "    return {\"documents\": documents, \"question\": question}\n",
                "\n",
                "def grade_documents(state):\n",
                "    \"\"\"\n",
                "    Determines whether the retrieved documents are relevant to the question.\n",
                "    \"\"\"\n",
                "    print(\"---CHECK DOCUMENT RELEVANCE---\")\n",
                "    question = state[\"question\"]\n",
                "    documents = state[\"documents\"]\n",
                "    \n",
                "    # LLM with JSON output to score relevancy\n",
                "    prompt = ChatPromptTemplate.from_template(\n",
                "        \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
                "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
                "        Here is the user question: {question} \\n\n",
                "        If the document contains keyword(s) or semantic meaning useful to the question, grade it as relevant. \\n\n",
                "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n\n",
                "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\"\n",
                "    )\n",
                "    chain = prompt | llm | JsonOutputParser()\n",
                "    \n",
                "    filtered_docs = []\n",
                "    for d in documents:\n",
                "        score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
                "        grade = score[\"score\"]\n",
                "        if grade == \"yes\":\n",
                "            print(\"   - Grade: RELEVANT\")\n",
                "            filtered_docs.append(d)\n",
                "        else:\n",
                "            print(\"   - Grade: NOT RELEVANT\")\n",
                "            continue\n",
                "            \n",
                "    return {\"documents\": filtered_docs, \"question\": question}\n",
                "\n",
                "def generate(state):\n",
                "    \"\"\"\n",
                "    Generate answer\n",
                "    \"\"\"\n",
                "    print(\"---GENERATE---\")\n",
                "    question = state[\"question\"]\n",
                "    documents = state[\"documents\"]\n",
                "    \n",
                "    prompt = ChatPromptTemplate.from_template(\n",
                "        \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
                "        Question: {question} \n",
                "        Context: {context} \n",
                "        Answer:\"\"\"\n",
                "    )\n",
                "    chain = prompt | llm_gen | StrOutputParser()\n",
                "    generation = chain.invoke({\"context\": documents, \"question\": question})\n",
                "    return {\"generation\": generation}\n",
                "\n",
                "def transform_query(state):\n",
                "    \"\"\"\n",
                "    Transform the query to produce a better question.\n",
                "    \"\"\"\n",
                "    print(\"---TRANSFORM QUERY---\")\n",
                "    question = state[\"question\"]\n",
                "    count = state.get(\"reformulated_count\", 0) + 1\n",
                "    \n",
                "    if count > 3: # Limit loops\n",
                "        print(\"   - Max retries reached, stopping.\")\n",
                "        return {\"question\": question, \"reformulated_count\": count}\n",
                "\n",
                "    prompt = ChatPromptTemplate.from_messages([\n",
                "        (\"system\", \"You are a helper that re-writes questions to improve retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"),\n",
                "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question. Output only the improved question string.\")\n",
                "    ])\n",
                "    chain = prompt | llm_gen | StrOutputParser()\n",
                "    better_question = chain.invoke({\"question\": question})\n",
                "    print(f\"   - Modified: {better_question}\")\n",
                "    return {\"question\": better_question, \"reformulated_count\": count}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Conditional Edges"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def decide_to_generate(state):\n",
                "    \"\"\"\n",
                "    Determines whether to generate an answer, or re-generate a question.\n",
                "    \"\"\"\n",
                "    filtered_documents = state[\"documents\"]\n",
                "    count = state.get(\"reformulated_count\", 0)\n",
                "    \n",
                "    if not filtered_documents and count <= 3:\n",
                "        # No relevant documents found, regenerate question\n",
                "        return \"transform_query\"\n",
                "    else:\n",
                "        # We have relevant documents, so generate answer\n",
                "        return \"generate\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "workflow = StateGraph(GraphState)\n",
                "\n",
                "# Add Nodes\n",
                "workflow.add_node(\"retrieve\", retrieve)\n",
                "workflow.add_node(\"grade_documents\", grade_documents)\n",
                "workflow.add_node(\"generate\", generate)\n",
                "workflow.add_node(\"transform_query\", transform_query)\n",
                "\n",
                "# Add Edges\n",
                "workflow.set_entry_point(\"retrieve\")\n",
                "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
                "\n",
                "# Conditional Edge\n",
                "workflow.add_conditional_edges(\n",
                "    \"grade_documents\",\n",
                "    decide_to_generate,\n",
                "    {\n",
                "        \"transform_query\": \"transform_query\",\n",
                "        \"generate\": \"generate\",\n",
                "    },\n",
                ")\n",
                "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
                "workflow.add_edge(\"generate\", END)\n",
                "\n",
                "# Compile\n",
                "app = workflow.compile()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualise Graph (Optional)\n",
                "Requires `graphviz` usually, but we can print the structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "try:\n",
                "    display(Image(app.get_graph().draw_mermaid_png()))\n",
                "except Exception:\n",
                "    print(\"Graph visualization requires 'grandalf' or 'graphviz' extra dependencies.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run the Agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = {\"question\": \"What is Multi-Head Attention?\"}\n",
                "for output in app.stream(inputs):\n",
                "    for key, value in output.items():\n",
                "        print(f\"Finished Node: {key}\")\n",
                "\n",
                "# Final Answer\n",
                "print(\"\\n--- Final Result ---\")\n",
                "print(value.get(\"generation\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}