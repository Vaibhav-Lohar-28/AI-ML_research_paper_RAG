{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. Core RAG Pipeline\n",
                "\n",
                "This notebook connects the pieces: **Vector Store** (Knowledge) + **Llama** (Reasoning).\n",
                "\n",
                "**Steps:**\n",
                "1.  **Load Vector Store**: Connect to the persistent ChromaDB.\n",
                "2.  **Setup Retriever**: configure how we fetch documents.\n",
                "3.  **Setup LLM**: Connect to Ollama (Llama 3).\n",
                "4.  **Create Chain**: Build the RAG retrieval chain.\n",
                "5.  **Test**: Ask questions and verify citations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_chroma import Chroma\n",
                "from langchain_ollama import ChatOllama\n",
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# Configuration\n",
                "DB_DIR = \"data/chroma_db\"\n",
                "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
                "LLM_MODEL = \"llama3\" # Ensure this matches your `ollama list`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Vector Store & Retriever"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embedding_model = HuggingFaceEmbeddings(\n",
                "    model_name=EMBEDDING_MODEL,\n",
                "    model_kwargs={'device': 'cpu'}, \n",
                "    encode_kwargs={'normalize_embeddings': True}\n",
                ")\n",
                "\n",
                "vector_store = Chroma(\n",
                "    persist_directory=DB_DIR,\n",
                "    embedding_function=embedding_model\n",
                ")\n",
                "\n",
                "retriever = vector_store.as_retriever(\n",
                "    search_type=\"similarity\",\n",
                "    search_kwargs={\"k\": 5}\n",
                ")\n",
                "\n",
                "print(\"Retriever ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup LLM (Ollama)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatOllama(\n",
                "    model=LLM_MODEL,\n",
                "    temperature=0.1, # Keep temperature low for factual answers\n",
                ")\n",
                "print(f\"Connected to {LLM_MODEL}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build RAG Chain\n",
                "We use a standard \"stuff\" chain which stuffs all retrieved documents into the context window."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt = (\n",
                "    \"You are an expert research assistant specializing in AI, Machine Learning, and Data Science. \"\n",
                "    \"Use the retrieved context below to answer the user's question. \"\n",
                "    \"If the answer is not in the context, say you don't know. \"\n",
                "    \"Keep answers technical and concise.\\n\\n\"\n",
                "    \"Context:\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", system_prompt),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
                "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask(query):\n",
                "    print(f\"\\nQuestion: {query}...\")\n",
                "    response = rag_chain.invoke({\"input\": query})\n",
                "    \n",
                "    print(\"\\nAnswer:\")\n",
                "    print(response[\"answer\"])\n",
                "    \n",
                "    print(\"\\nSources:\")\n",
                "    for i, doc in enumerate(response[\"context\"]):\n",
                "        print(f\"- {doc.metadata.get('source')} (Page {doc.metadata.get('page')})\")\n",
                "\n",
                "# Example Question (Assumes Attention Is All You Need is loaded)\n",
                "ask(\"What are the benefits of self-attention mechanisms?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ask(\"Explain the difference between encoder and decoder blocks in the Transformer.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}