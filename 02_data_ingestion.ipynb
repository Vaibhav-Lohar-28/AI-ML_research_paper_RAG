{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Data Ingestion & Indexing Pipeline\n",
                "\n",
                "This notebook handles the ETL (Extract, Transform, Load) process for our Agentic RAG system.\n",
                "\n",
                "**Steps:**\n",
                "1.  **Load PDFs**: Read research papers from `data/papers`.\n",
                "2.  **Preprocess**: Clean text and handle formatting artifacts.\n",
                "3.  **Chunk**: Split text into semantic chunks suitable for retrieval.\n",
                "4.  **Embed**: Convert chunks into vector embeddings.\n",
                "5.  **Index**: Store embeddings in a local Vector Database (ChromaDB)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "from typing import List\n",
                "\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_chroma import Chroma\n",
                "from langchain_core.documents import Document\n",
                "\n",
                "# Configuration\n",
                "DATA_DIR = \"data/papers\"\n",
                "DB_DIR = \"data/chroma_db\"\n",
                "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
                "CHUNK_SIZE = 1000\n",
                "CHUNK_OVERLAP = 200"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Helper: Download Sample Paper\n",
                "If the directory is empty, let's download \"Attention Is All You Need\" as a test case."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_sample_paper():\n",
                "    url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
                "    target_path = os.path.join(DATA_DIR, \"1706.03762v5.pdf\")\n",
                "    \n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        os.makedirs(DATA_DIR)\n",
                "        \n",
                "    if not os.listdir(DATA_DIR):\n",
                "        print(f\"Downloading sample paper to {target_path}...\")\n",
                "        response = requests.get(url)\n",
                "        with open(target_path, 'wb') as f:\n",
                "            f.write(response.content)\n",
                "        print(\"Download complete.\")\n",
                "    else:\n",
                "        print(\"Papers found in directory, skipping download.\")\n",
                "\n",
                "download_sample_paper()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Documents\n",
                "We use `PyPDFLoader` to extract text from all PDF files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_documents(directory: str) -> List[Document]:\n",
                "    documents = []\n",
                "    for filename in os.listdir(directory):\n",
                "        if filename.endswith(\".pdf\"):\n",
                "            file_path = os.path.join(directory, filename)\n",
                "            print(f\"Loading {filename}...\")\n",
                "            loader = PyPDFLoader(file_path)\n",
                "            documents.extend(loader.load())\n",
                "    print(f\"Loaded {len(documents)} pages from {len(os.listdir(directory))} files.\")\n",
                "    return documents\n",
                "\n",
                "raw_documents = load_documents(DATA_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Split Text (Chunking)\n",
                "We use `RecursiveCharacterTextSplitter`. For research papers, keeping context is key, so we use a relatively large chunk size with overlap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=CHUNK_SIZE,\n",
                "    chunk_overlap=CHUNK_OVERLAP,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
                ")\n",
                "\n",
                "chunks = text_splitter.split_documents(raw_documents)\n",
                "print(f\"Created {len(chunks)} chunks from original documents.\")\n",
                "\n",
                "# Preview a random chunk\n",
                "if chunks:\n",
                "    print(\"\\n--- Chunk Preview ---\")\n",
                "    print(chunks[0].page_content[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Embed Loading\n",
                "We use `sentence-transformers/all-mpnet-base-v2` via HuggingFaceEmbeddings. This runs locally on CPU/GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading embedding model... (this may take a moment first time)\")\n",
                "embedding_model = HuggingFaceEmbeddings(\n",
                "    model_name=EMBEDDING_MODEL,\n",
                "    model_kwargs={'device': 'cpu'}, # Use 'cuda' if you have a GPU\n",
                "    encode_kwargs={'normalize_embeddings': True}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Vector Store Indexing (ChromaDB)\n",
                "We persist the database to disk so we don't have to re-index every time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Chroma and add documents\n",
                "# If the DB already exists, this will load it and append new documents\n",
                "# To reset, delete the 'data/chroma_db' folder manually\n",
                "vector_store = Chroma.from_documents(\n",
                "    documents=chunks,\n",
                "    embedding=embedding_model,\n",
                "    persist_directory=DB_DIR\n",
                ")\n",
                "\n",
                "print(f\"\\nIndexed {len(chunks)} chunks into ChromaDB at '{DB_DIR}'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Retrieval\n",
                "Let's verify the index works by asking a simple question related to the papers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"What is the main advantage of the Transformer model?\"\n",
                "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
                "\n",
                "results = retriever.invoke(query)\n",
                "\n",
                "print(f\"Query: {query}\\n\")\n",
                "for i, doc in enumerate(results):\n",
                "    print(f\"[Result {i+1}] Source: {doc.metadata.get('source', 'Unknown')} | Page: {doc.metadata.get('page', 'Unknown')}\")\n",
                "    print(doc.page_content[:200] + \"...\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}